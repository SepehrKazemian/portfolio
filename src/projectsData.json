[
  {
    "id": 1,
    "title": "Semantic Resume Tailor",
    "url": "https://github.com/SepehrKazemian/cv_tailor",
    "content": [
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "This project focuses on enhancing how a candidate's resume presents their skills for specific job applications. The system automatically tailors the resume\u2019s skills section by analyzing both the candidate\u2019s background and the requirements listed in a particular job posting. By intelligently highlighting the most relevant qualifications, it increases the chances of standing out to recruiters and hiring managers."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "Behind the scenes, the system leverages a dynamic knowledge graph architecture built with Neo4j, organizing skills hierarchically across three layers: specific competencies, broader categories, and overarching domains. A retrieval-augmented generation (RAG) approach is used to parse and enrich skill information, combining traditional fuzzy matching with advanced embedding models and LLM-powered standardization agents. Skills from both the candidate\u2019s profile and the job description are extracted and canonically aligned using techniques like semantic search, vector similarity, and AI-driven naming tools to ensure consistent, professional terminology across diverse sources."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "To personalize the resume, the system employs intelligent agent flows that score and reason over the structured skill graph. Match relevance is calculated not only at the granular skill level but also cascaded upwards through categories and domains, allowing a more strategic view of a candidate\u2019s fit for a role. Using insights from this LLM-augmented graph reasoning, the system dynamically assembles a tailored skills section, optimizing the resume\u2019s impact with high-precision, role-specific qualifications."
          }
        ]
      }
    ]
  },
  {
    "id": 2,
    "title": "Book2Audio Voice Cloning",
    "url": "https://github.com/SepehrKazemian/book_2_audio",
    "content": [
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "This project transforms traditional PDF books into fully narrated audiobooks through an automated, AI-powered pipeline. It intelligently extracts text, corrects errors and formatting inconsistencies, and generates natural, speaker-consistent audio. From a simple PDF, the system produces a polished audiobook experience with minimal manual intervention."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "The pipeline integrates advanced AI techniques across multiple stages. Text extraction combines precise parsing with OCR fallback for scanned files, while a fine-tuned StableLM-Zephyr-3B model, enhanced with Low-Rank Adaptation (LoRA), corrects spelling, structure, and OCR-related noise. A custom training set, derived from augmented Wikipedia content, allows the model to specialize in domain-specific text cleaning. Audiobook synthesis is powered by Coqui TTS\u2019s XTTSv2 model, producing high-fidelity narration efficiently through GPU-parallelization. Quality assurance uses both fuzzy matching and semantic similarity via embedding models to validate alignment with the original content."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "The system architecture follows best-in-class software practices. Each stage of the pipeline \u2014 extraction, cleaning, correction, and synthesis \u2014 is modularized for maintainability and scalability. Error handling, structured logging, and type safety are embedded throughout. Configuration management abstracts runtime parameters for flexible deployments. Built using leading ML and AI frameworks, the project ensures a robust, reproducible workflow, ready to scale from individual users to production environments."
          }
        ]
      }
    ]
  },
  {
    "id": 3,
    "title": "Realtime Raspberry PI MAANG Scraper",
    "url": "https://github.com/SepehrKazemian/MAANG_scraper_pi",
    "content": [
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "I built a fully automated job monitoring system that continuously tracks new job postings from companies like Meta, Microsoft, Google, and DeepMind. The system runs independently on a Raspberry Pi, leveraging dynamic web scraping and API access to capture postings in real-time and deliver notifications via Telegram. Designed for reliability and low-maintenance operation, it combines robustness with lightweight deployment, ensuring consistent visibility into new opportunities without manual searching."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "From an engineering perspective, this project showcases full-stack automation skills relevant to AI infrastructure work. I implemented asynchronous scraping pipelines using pyppeteer and API integrations, incorporating TOR network handling, browser lifecycle management, and fault-tolerant pagination scraping. To ensure resilience, I engineered process supervision with PID tracking, error isolation, and auto-recovery strategies. Although no AI models were involved, the system demonstrates critical backend skills for building reliable, real-world pipelines \u2014 skills directly transferable to maintaining production ML workflows, such as managing inference jobs, data collection streams, and low-footprint cloud or edge deployments."
          }
        ]
      }
    ]
  },
  {
    "id": 4,
    "title": "LLM RAG Pipeline",
    "url": null,
    "content": [
      {
        "type": "h3",
        "text": "Use Case"
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "This project involved building a production-grade Retrieval-Augmented Generation (RAG) system for insurance document summarization and intelligent question answering. It was designed to support real-time and batch inference using modular components deployed across a cloud-native architecture."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "The system leveraged LangChain, LlamaIndex, and LangGraph to orchestrate QA and summarization tasks with LLM-as-a-Service capabilities. Content from third-party sources was parsed, chunked, and embedded along with metadata such as page number, topic, and document ID into both Qdrant and Azure Vector Search, as well as a graph database. A stacked retriever architecture was implemented combining transformer-based semantic search with FAISS, supported by multi-query expansion, context filtering, and deduplication logic to optimize precision."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "Dynamic routing logic directed different query types and content to specialized LLMs based on metadata, enabling efficient load balancing and accuracy gains. The system utilized Chain-of-Thought prompting and RAGAS for answer generation and evaluation, while LLM-as-a-judge techniques ensured hallucination detection, reranking, and response groundedness. A real-time user feedback system captured corrections and scores, which were automatically funneled into an online fine-tuning loop for continual learning. The full stack was deployed on Azure with Kubernetes, FastAPI, and Streamlit, integrating monitoring via Langfuse, Datadog, and Azure Monitor."
          }
        ]
      }
    ]
  },
  {
    "id": 5,
    "title": "Document Classifier",
    "url": null,
    "content": [
      {
        "type": "h3",
        "text": "Use Case"
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "This system was designed to classify 30 types of insurance and business documents in real-time, balancing accuracy, speed, and cost efficiency. A custom fine-tuned EfficientNet model performed image-based classification using RGB-rendered inputs. For cases with lower vision confidence, an NLP fallback based on a RoBERTa model (trained on OCR-extracted text) provided a secondary decision layer. Regex-based rules served as a final check to reinforce classification logic for edge cases."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "Documents were routed dynamically through this layered pipeline \u2014 vision, NLP, then regex \u2014 depending on confidence scores. The system achieved 97% accuracy in production and supported both real-time and batch uploads. Deployed on Azure, it used FastAPI, Blob Storage, App Service, and Redis queues managed under AKS. A client-facing dashboard allowed for file uploads, OCR correction, and retriggering classification in real-time. Periodic retraining was driven by corrected user labels collected through the interface, integrated into the training loop using Hugging Face and FastAPI Cron jobs. The system supported long-term model improvement via continual learning and adaptive retraining."
          }
        ]
      }
    ]
  },
  {
    "id": 6,
    "title": "Impairment Extractor",
    "url": null,
    "content": [
      {
        "type": "h3",
        "text": "Use Case"
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "Built to automate the extraction of impairment-related data fields from varied insurance documents, this system combined OCR, NLP, and rule-based logic for robust, production-ready field extraction. The pipeline handled multiple formats including scanned forms, digital PDFs, and Word documents."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "Text and positional metadata were extracted using Tesseract, PyMuPDF, PDFMiner, and bounding box libraries. Handwritten regex rules seeded the initial pipeline and were later augmented by a RoBERTa model fine-tuned to identify and extract target fields. The system achieved 96% extraction accuracy across formats. A real-time dashboard enabled uploads, correction of OCR errors, and rerunning classification dynamically."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "The entire system was containerized with Docker and deployed to Azure Kubernetes Service (AKS), supported by Redis for queueing and Azure Functions for triggering. File management was handled via Blob Storage, and a responsive web UI was served through Azure App Service. Designed with modularity and extensibility in mind, the system supports both real-time and batch modes, and enables hybrid rule/ML-based extraction."
          }
        ]
      }
    ]
  },
  {
    "id": 7,
    "title": "Pricing Analytics",
    "url": null,
    "content": [
      {
        "type": "h3",
        "text": "Use Case"
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "This project focused on forecasting future healthcare-related costs based on insurance claim data and macroeconomic indicators. Working with a healthcare insurer, I built and deployed predictive models using ARIMA, SARIMA, LSTM, and rolling average ensembles to model future client spending."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "External data such as CPI and inflation rates were ingested via APIs and feature-engineered alongside lag windows, seasonality markers, and demographic features. A fully automated ETL pipeline was developed using SQL, Airflow, Pandas, and dbt to ensure consistent delivery of clean, model-ready datasets."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "Model predictions and SHAP-based explainability were delivered via interactive dashboards built in Power BI, Streamlit, and Plotly Dash, supporting executive-level pricing decisions. The MLOps framework included retraining schedules, CI/CD (GitHub Actions), and containerized model serving. The final product delivered real-time insights, enabled scenario planning, and reduced forecast error across multiple regions."
          }
        ]
      }
    ]
  },
  {
    "id": 8,
    "title": "Data Anonymization",
    "url": null,
    "content": [
      {
        "type": "h3",
        "text": "Use Case"
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "This system anonymized semi-structured medical JSON documents by detecting and redacting personal identifiable information (PII) across both keys and values, preserving document structure and supporting auditability."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "A custom-built recursive parser traversed deeply nested JSON objects using BFS/DFS to create analyzable trees. Regex-based rules captured basic PII types, while a BERT classifier trained on path-to-value contexts handled nuanced redaction cases. Redacted values were substituted using per-file randomized mapping to ensure referential consistency, with mappings stored for versioned audit recovery."
          }
        ]
      },
      {
        "type": "paragraph",
        "parts": [
          {
            "type": "text",
            "value": "Processed documents were reconstructed into fully valid JSON, maintaining formatting integrity. Users could upload documents in real time via a dashboard, where anonymization occurred instantly or via batch processing depending on workload. The system was deployed on Azure with support for high-throughput processing, leveraging Docker, Azure Functions, Redis, and Blob Storage. It was validated across complex schemas and edge cases, with built-in support for feedback, fine-tuning, and compliance audits."
          }
        ]
      }
    ]
  }
]